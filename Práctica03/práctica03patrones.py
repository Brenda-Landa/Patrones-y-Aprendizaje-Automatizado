# -*- coding: utf-8 -*-
"""Práctica03Patrones

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10dbDMLV-pOng3aPNLdE0vfnH4mwfcnoi
"""

from keras.datasets import mnist
import numpy as np

def balanced_mnist_set():
  (x_train, y_train), (x_test, y_test) = mnist.load_data()

  ## Normalización de datos
  x_train = x_train / 255
  x_test = x_test / 255

  # Se obtiene una muestra con igual número de: cincos vs no cincos
  # para el conjunto de entrenamiento y el de test
  idx_y_train_5 = np.where(y_train == 5)[0]
  idx_y_train_n5 = np.where(y_train != 5)[0]
  idx_y_train_n5 = np.random.choice(idx_y_train_n5, idx_y_train_5.shape[0], replace=False)

  idx_y_train = np.concatenate([idx_y_train_5, idx_y_train_n5])
  np.random.shuffle(idx_y_train)

  x_train = x_train[idx_y_train]
  y_train = y_train[idx_y_train]
  #print(x_train.shape)

  idx_y_test_5 = np.where(y_test == 5)[0]
  idx_y_test_n5 = np.where(y_test != 5)[0]
  idx_y_test_n5 = np.random.choice(idx_y_test_n5, idx_y_test_5.shape[0], replace=False)

  idx_y_test = np.concatenate([idx_y_test_5, idx_y_test_n5])
  np.random.shuffle(idx_y_test)

  x_test = x_test[idx_y_test]
  y_test = y_test[idx_y_test]
  #print(x_test.shape)

  y_train_binary = (y_train == 5).astype(int)
  y_test_binary = (y_test == 5).astype(int)
  return (x_train, y_train_binary), (x_test, y_test_binary)

"""# Modelo con 1 neurona (función de activación: lineal)"""

import matplotlib.pyplot as plt

from keras.models import Sequential
from keras.layers import Flatten, Dense
from keras.callbacks import EarlyStopping
from sklearn.metrics import confusion_matrix, \
precision_score, recall_score,f1_score

(x_train, y_train_binary), (x_test, y_test_binary) = balanced_mnist_set()
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(1, activation='sigmoid')
])

callback = EarlyStopping(monitor='accuracy', patience=3, restore_best_weights=True)
model.compile(loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(x_train, y_train_binary, epochs=200, verbose=0,
                    batch_size=500, validation_split=0.2, callbacks=[callback])

loss_train = history.history['loss']
loss_val = history.history['val_loss']
accuracy_train = history.history['accuracy']
accuracy_val = history.history['val_accuracy']
epochs = range(1, len(loss_train) + 1)

# Loss
plt.figure(figsize=(6, 6))
plt.plot(epochs, loss_train, label='loss')
plt.plot(epochs, loss_val, label='val_loss')
plt.title('Loss')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.grid(True)
plt.show()

# Accuracy
plt.figure(figsize=(6, 6))
plt.plot(epochs, accuracy_train, label='accuracy')
plt.plot(epochs, accuracy_val, label='val_accuracy')
plt.title('Accuracy')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend()
plt.grid(True)
plt.show()


y_pred = (model.predict(x_test, verbose=0) >= 0.5).astype(int)[:,0]

print(precision_score(y_test_binary, y_pred),
      recall_score(y_test_binary, y_pred),
      f1_score(y_test_binary, y_pred),
      precision_score(y_test_binary, y_pred, pos_label=0),
      recall_score(y_test_binary, y_pred, pos_label=0),
      f1_score(y_test_binary, y_pred, pos_label=0))

"""# Modelo con 2 capas
  * Capa 1: 2 neuronas, función de activación: ReLu.
  * Capa 2: 1 neurona, función de activación: Sigmoide
"""

import matplotlib.pyplot as plt

from keras.models import Sequential
from keras.layers import Flatten, Dense
from keras.callbacks import EarlyStopping
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import confusion_matrix, \
precision_score, recall_score,f1_score

(x_train, y_train_binary), (x_test, y_test_binary) = balanced_mnist_set()
model = Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(2, activation='relu'), ## Primera capa, dos neuronas
    Dense(1, activation='sigmoid') ## Segunda capa, 1 neurona
])

## Usando Early Stopping para regresar a la época con mejor accuracy
## si accuracy baja durante 3 épocas consecutivas.
callback = EarlyStopping(monitor='accuraaccuracy:", accuracy_score(y_test_binary, y_pred),accuracy:", accuracy_score(y_test_binary, y_pred),cy', patience=3, restore_best_weights=True)
model.compile(loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(x_train, y_train_binary, epochs=200, verbose=0,
                    batch_size=500, validation_split=0.2, callbacks=[callback])

loss_train = history.history['loss']
loss_val = history.history['val_loss']
accuracy_train = history.history['accuracy']
accuracy_val = history.history['val_accuracy']
epochs = range(1, len(loss_train) + 1)

# Loss
plt.figure(figsize=(6, 6))
plt.plot(epochs, loss_train, label='loss')
plt.plot(epochs, loss_val, label='val_loss')
plt.title('Loss')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.grid(True)
plt.show()

# Accuracy
plt.figure(figsize=(6, 6))
plt.plot(epochs, accuracy_train, label='accuracy')
plt.plot(epochs, accuracy_val, label='val_accuracy')
plt.title('Accuracy')
plt.xlabel('epoch')
plt.ylabel('accuracy')
plt.legend()
plt.grid(True)
plt.show()


y_pred = (model.predict(x_test, verbose=0) >= 0.5).astype(int)[:,0]

## Se imprimen los valores de precision, recall y f1-score para las clases 0 y 1 con los tests.
## Falta imprimir accuracy.
print("precision_score:", precision_score(y_test_binary, y_pred),
      "recall_score:", recall_score(y_test_binary, y_pred),
      "f1_score:", f1_score(y_test_binary, y_pred),
      "\nprecision_score:", precision_score(y_test_binary, y_pred, pos_label=0),
      "recall_score:", recall_score(y_test_binary, y_pred, pos_label=0),
      "f1_score:", f1_score(y_test_binary, y_pred, pos_label=0),
      "\naccuracy:", accuracy_score(y_test_binary, y_pred))

# Obtener las predicciones del modelo
y_pred = (model.predict(x_test) >= 0.5).astype(int)[:, 0]
errores_indices = np.nonzero(y_pred != y_test_binary)[0]
plt.figure(figsize=(8,8))
for i, incorrect in enumerate(np.random.choice(errores_indices, 80, False)):
    plt.subplot(10,8,i+1)
    plt.imshow(x_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')
    plt.title("P={}, C={}".format(y_pred[incorrect], y_test_binary[incorrect]))
    plt.tick_params(axis='both',which='both',bottom='off',left='off',labelbottom='off',labelleft='off')
plt.tight_layout()
plt.show()

##Curva ROC

##Obtener las probabilidades predichas
#y_probabilidades = model.predict_proba(x_train)[:, 1]

##Calcular la curva ROC
##fpr, tpr, _ = roc_curve(y_test_binary, y_pred)
##roc_auc = auc(fpr, tpr)

##Graficar la curva ROC
##plt.figure(figsize=(10,6))
##plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Curva ROC (AUC = {roc_auc: .2f})')
##plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
##plt.xlabel('Tasa de Falsos Positivos (FPR)')
##plt.ylabel('Tasa de Verdaderos Positivos (TPR)')
##plt.title('Curva ROC')
##plt.legend(loc='lower right')
##plt.show()